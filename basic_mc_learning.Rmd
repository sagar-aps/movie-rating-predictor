---
title: "Machine-Learning_basics"
author: "Sagar A"
date: "10/4/2020"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r packages}
install.packages("caret")
library(caret)
library(dplyr)
library(dslabs)
```

## Predicting sex based on height

We will try to predict sex based on height

```{r pressure, echo=FALSE}
# define the outcome and predictors
y <- heights$sex
x <- heights$height

# generate training and test sets
set.seed(2, sample.kind = "Rounding") # if using R 3.5 or earlier, remove the sample.kind argument
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

test_set <- heights[test_index, ]
train_set <- heights[-test_index, ]
```
# Guess the outcome

Our functions developed for machine learning,such as those in the caret package, require or recommend that categorical outcomes be coded as factors.
```{r guess, echo=FALSE}

y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE)%>% 
  factor(levels = levels(test_set$sex))

mean(y_hat==test_set$sex)
```

This sounds correct. We guessed and have the result right 50% of the time.But we can explore the data to see that males are slightly taller than females.

```{r }
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
```


Lets predict male if height is within two sd of male average


```{r guess}
y_hat <- ifelse(x>69.3-2*3.61,"Male","Female")%>% factor(levels=levels(test_set$sex))
mean(y_hat==test_set$sex)
```

Much better accuracy!

Lets see which is the best cutoff to predict Male. 

```{r guess}
cutoff <- seq(61, 70)
accuracy <- sapply(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  mean(y_hat == train_set$sex)
})
data.frame(cutoff, accuracy) %>% 
  ggplot(aes(cutoff, accuracy)) + 
  geom_point() + 
  geom_line() 
max(accuracy)
```
```{r cutoff}

best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff

```





```{r cutoff}

y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
y_hat <- factor(y_hat)
mean(y_hat == test_set$sex)

```

OVerall accuracy is very high. Lets look at the results more closely.


```{r}
table(predicted = y_hat, actual = test_set$sex)

```

This shows a problem. 69/(69+50) actual females were predicted male.


```{r}
test_set %>% 
  group_by(sex) %>%
  summarise(accuracy = mean(y_hat==sex))

```



This because of ##Prevelance .

There are many more males than there are females.

## Specificity (recall), Prevelance() and hybrid indicators
```{r, echo=FALSE}
htmltools::includeHTML("table_recall.html")
```

It is useful to have a one-number summary, for example for optimization purposes.

$$ F1 score = \frac{1}{\frac{1}{2}\left(\frac{1}{\mbox{recall}} + 
    \frac{1}{\mbox{precision}}\right) }$$


The F_meas function in the caret package calculates this score.


```{r}
cutoff <- seq(61, 70)
F_1 <- sapply(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  F_meas(data = y_hat, reference = factor(train_set$sex))
})

data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff, F_1)) + 
  geom_point() + 
  geom_line()


```





